{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **JIVE: Joint and Individual Variation Explained**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JIVE (Joint and Individual Variation Explained) is a dimensional reduction algorithm that can be used when there are multiple data matrices (data blocks). The multiple data block setting means there are $K$ different data matrices, with the same number of observations $n$ and (possibly) different numbers of variables ($d_1, \\dots, d_k$). JIVE finds modes of variation which are common (joint) to all $K$ data blocks and modes of individual variation which are specific to each block. For a detailed discussion of JIVE see [Angle-Based Joint and Individual Variation Explained](https://arxiv.org/pdf/1704.02060.pdf).[^1]\n",
    "\n",
    "For a concrete example, consider a two block example from a medical study. Suppose there are $n=500$ patients (observations). For each patient we have $d_1 = 100$ bio-medical variables (e.g. hit, weight, etc). Additionally we have $d_2 = 10,000$ gene expression measurements for each patient.\n",
    "\n",
    "## **The JIVE decomposition**\n",
    "\n",
    "Suppose we have $K$ data data matrices (blocks) with the same number of observations, but possibly different numbers of variables; in particular let $X^{(1)}, \\dots, X^{(K)}$ where $X^{(k)} \\in \\mathbb{R}^{n \\times d_k}$. JIVE will then decompose each matrix into three components: joint signal, individual signal and noise\n",
    "\n",
    "\\begin{equation}\n",
    "X^{(k)} = J^{(k)} + I^{(k)} + E^{(k)}\n",
    "\\end{equation}\n",
    "\n",
    "where $J^{(k)}$ is the joint signal estimate, $I^{(k)}$ is the individual signal estimate and $E^{(k)}$ is the noise estimate (each of these matrices must the same shape as the original data block: $\\mathbb{R}^{n \\times d_k}$). Note: **we assume each data matrix** $X^{(k)}$ **has been column mean centered**.\n",
    "\n",
    "\n",
    "The matrices satisfy the following constraints:\n",
    "\n",
    "1. The joint matrices have a common rank: $rk(J^{(k)}) = r_{joint}$ for $k=1, \\dots, K$.\n",
    "2. The individual matrices have block specific ranks $rk(I^{(k)}) = r_{individual}^{(k)}$.\n",
    "3. The columns of the joint matrices share a common space called the joint score space (a subspace of $\\mathbb{R}^n$); in particular the $\\text{col-span}(J^{(1)}) = \\dots = \\text{col-span}(J^{(K)})$ (hence the name joint).\n",
    "4. Each individual spaces score subspace (of $\\mathbb{R}^n$) is orthogonal to the the joint space; in particular $\\text{col-span}(J^{(k)}) \\perp \\text{col-span}(I^{(k)})$ for $k=1, \\dots, K$.\n",
    "\n",
    "Note that JIVE may be more natural if we think about data matrices subspaces of $\\mathbb{R}^n$ (the score space perspective). Typically we think of a data matrix as $n$ points in $\\mathbb{R}^d$. The score space perspective views a data matrix as $d$ vectors in $\\mathbb{R}^n$ (or rather the span of these vectors). One important consequence of this perspective is that it makes sense to related the data blocks in score space (e.g. as subspaces of $\\mathbb{R}^n$) since they share observtions.\n",
    "\n",
    "## Quantities of interest\n",
    "\n",
    "There are a number of potential quantities of interest depending on the application. For example the user may be interested in the full matrices $J^{(k)}$ and/or $I^{(k)}$. By construction these matrices are not full rank and we may also be interested in their singular value decomposition which we define as\n",
    "\n",
    "\\begin{align}\n",
    "& U^{(k)}_{joint}, D^{(k)}_{joint}, V^{(k)}_{joint} = \\text{rank } r_{joint} \\text{ SVD of } J^{(k)} \\\\\n",
    "& U^{(k)}_{individual}, D^{(k)}_{individual}, V^{(k)}_{individual} = \\text{rank } r_{individual}^{{k}} \\text{ SVD of } I^{(k)}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "One additional quantity of interest is $U_{joint} \\in \\mathbb{R}^{n \\times r_{joint}}$ which is an orthogonal basis of $\\text{col-span}(J^{(k)})$. This matrix is produced from an intermediate JIVE computation. \n",
    "\n",
    "## **PCA analogy**\n",
    "We give a brief discussion of the PCA/SVD decomposition (assuming the reading is already familiar).\n",
    "\n",
    "#### Basic decomposition\n",
    "Suppose we have a data matrix $X \\in \\mathbb{n \\times d}$. Assume that $X$ has been column mean centered and consider the SVD decomposition (this is PCA since we have mean centered the data):\n",
    "\n",
    "\\begin{equation}\n",
    "X = U D V^T.\n",
    "\\end{equation}\n",
    "where $U \\in \\mathbb{R}^{n \\times m}$, $D \\in \\mathbb{R}^{m \\times m}$ is diagonal, and $V \\in \\mathbb{R}^{d \\times m}$ with $m = min(n, d)$. Note $U^TU = V^TV = I_{m \\times m}$. \n",
    "\n",
    "Suppose we have decided to use a rank $r$ approximation. We can then decompose $X$ into a signal matrix ($A$) and an noise matrix ($E$)\n",
    "\n",
    "\\begin{equation}\n",
    "X = A + E,\n",
    "\\end{equation}\n",
    "where $A$ is the rank $r$ SVD approximation of $X$ i.e. \n",
    "\\begin{align}\n",
    "A := & U_{:, 1:r} D_{1:r, 1:r} V_{:, 1:r}^T \\\\\n",
    " = & \\widetilde{U}, \\widetilde{D} \\widetilde{V}^T\n",
    "\\end{align}\n",
    "The notation $U_{:, 1:r} \\in \\mathbb{R}^{n \\times r}$ means the first $r$ columns of $U$. Similarly we can see the error matrix is $E :=U_{:, r+1:n} D_{r+1:m, r_1:m} V_{:, r+1:d}^T$.\n",
    "\n",
    "#### Quantities of interest\n",
    "\n",
    "There are many ways to use a PCA/SVD decomposition. Some common quantities of interest include\n",
    "\n",
    "- The normalized scores: $\\widetilde{U} \\in \\mathbb{R}^{n \\times r}$\n",
    "- The unnormalized scores: $\\widetilde{U}\\widetilde{D} \\in \\mathbb{R}^{n \\times r}$\n",
    "- The loadings: $\\widetilde{V} \\in \\mathbb{R}^{d \\times r}$\n",
    "- The full signal approximation: $A \\in \\mathbb{R}^{n \\times d}$\n",
    "\n",
    "\n",
    "#### Scores and loadings\n",
    "\n",
    "For both PCA and JIVE we use the notation $U$ (scores) and $V$ (loadings). These show up in several places.\n",
    "\n",
    "We refer to all $U \\in \\mathbb{R}^{n \\times r}$ matrices as scores. We can view the $n$ rows of $U$ as representing the $n$ data points with $r$ derived variables (put differently, columns of $U$ are $r$ derived variables). The columns of $U$ are orthonormal: $U^TU = I_{r \\times r}$.\n",
    "\n",
    "Sometimes we may want $UD$ i.e scale the columns of $U$ by $D$ (the columns are still orthogonal). The can be useful when we want to represent the original data by $r$ variables. We refer to $UD$ as unnormalized scores.\n",
    "\n",
    "We refer to all $V\\in \\mathbb{R}^{d \\times r}$ matrices as loadings[^2]. The j$th$ column of $V$ gives the linear combination of the original $d$ variables which is equal to the j$th$ unnormalized scores (j$th$ column of $UD$). Equivalently, if we project the $n$ data points (rows of $X$) onto the j$th$ column of $V$ we get the j$th$ unnormalized scores. \n",
    "\n",
    "The typical geometric perspective of PCA is that the scores represent $r$ new derived variables. For example, if $r = 2$ we can look at a scatter plot that gives a two dimensional approximation of the data. In other words, the rows of the scores matrix are $n$ data points living in $\\mathbb{R}^r$. \n",
    "\n",
    "An alternative geometric perspective is the $r$ columns of the scores matrix are vectors living in $\\mathbb{R}^n$. The original $d$ variables span a subspace of $\\mathbb{R}^n$ given by $\\text{col-span}(X)$. The scores then span a lower dimensional subspace of $\\mathbb{R}^n$ that approximates $\\text{col-span}(X)$.\n",
    "\n",
    "The first perspective says PCA finds a lower dimensional approximation to a subspace in $\\mathbb{R}^d$ (spanned by the $n$ data points). The second perspective says PCA finds a lower dimensional approximation to a subspace in $\\mathbb{R}^n$ (spanned by the $d$ data points).\n",
    "\n",
    "## **JIVE operating in score space**\n",
    "\n",
    "For a data matrix $X$ let's call the span of the variables (columns) the *score subpace*, $\\text{col-span}(X) \\subset \\mathbb{R}^n$. Typically we think of a data matrix as $n$ points in $\\mathbb{R}^d$. The score space perspective reverses this and says a data matrix is $d$ points in $\\mathbb{R}^n$. When thinking in the score space it's common to consider about subspaces i.e. the span of the $d$ variables in $\\mathbb{R}^n$. In other words, if two data matrices have the same column span then their score subspaces are the same[^3].\n",
    "\n",
    "JIVE partitions the score space of each data matrix into three subspaces: joint, individual and noise. The joint score subspace for each data block is the same. The individual score subspace, however, is (possibly) different for each of the $K$ blocks. The k$th$ block's individual score subspace is orthogonal to the joint score subspace. Recall that the $K$ data matrices have the same number of observations ($n$) so it makes sense to think about how the data matrices relate to each other in score space.\n",
    "\n",
    "PCA partitions the score space into two subspaces: signal and noise (see above). For JIVE we might combine the joint and individual score subspaces and call this the signal score subspace.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "[^1]: Note this paper calls the algorithm AJIVE (angle based JIVE) however, we simply use JIVE. Additionally, the paper uses columns as observations in data matrices where as we use rows as observations.\n",
    "\n",
    "[^2]: For PCA we used tildes (e.g. $\\widetilde{U}$) to denote the \"partial\" SVD approximation however for the final JIVE decomposition we do not use tildes. This is intentional since for JIVE the SVD comes from the $I$ and $J$ matrices which are exactly rank $r$. Therefore we view this SVD as the \"full\" SVD.\n",
    "\n",
    "[^3]: This might remind the reader of TODO"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
